# SEEM-OCTA: Geometry-Informed Interactive Refinement for OCTA Vessel Segmentation
## ğŸ”¥ Overview

<p align="center">
  <img src="assets/Picture1.png" width="800"/>
</p>
---
SEEM-OCTA is an interactive segmentation framework for retinal OCTA vessel extraction...

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 2.0+](https://img.shields.io/badge/pytorch-2.0+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

This repository contains the implementation of **SEEM-OCTA with GIIR (Geometry-Informed Interactive Refinement)** for interactive vessel segmentation in Optical Coherence Tomography Angiography (OCTA) images.

## ğŸ¯ Key Features

- **Parameter-Efficient Fine-Tuning**: Uses LoRA adaptation with only ~1.2M trainable parameters (1.25% of total)
- **Interactive Refinement**: Achieves 0.91+ Dice score with just 3-4 clicks on average
- **3-Channel Input**: Utilizes FULL, ILM_OPL, and OPL_BM projections for robust segmentation
- **Two Training Strategies**:
  - **GIIR**: Geometry-Informed deterministic click selection using distance transforms
  - **Random**: SEEM baseline with random click selection


## ğŸ“ Project Structure

```
seem-octa/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ assets/
â”‚   â”œâ”€â”€ Picture1.png
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ seem/
â”‚       â””â”€â”€ focall_unicl_lang_demo.yaml
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ lora.py              # LoRA implementation
â”‚   â”‚   â”œâ”€â”€ losses.py            # Loss functions (Dice, Focal, Tversky)
â”‚   â”‚   â”œâ”€â”€ metrics.py           # Evaluation metrics
â”‚   â”‚   â””â”€â”€ model_utils.py       # Model loading utilities
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ dataset.py           # OCTADataset class
â”‚   â”‚   â””â”€â”€ point_generators.py  # Point generation strategies
â”‚   â”œâ”€â”€ strategies/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py              # Base click strategy interface
â”‚   â”‚   â”œâ”€â”€ giir.py              # GIIR deterministic clicks
â”‚   â”‚   â””â”€â”€ random_clicks.py     # SEEM random clicks
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ trainer.py           # Unified trainer class
â”‚   â”‚   â””â”€â”€ config.py            # Training configuration
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ evaluator.py         # Evaluation and comparison
â”‚   â””â”€â”€ visualization/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ plotting.py          # Visualization utilities
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py                 # Unified training script
â”‚   â”œâ”€â”€ evaluate.py              # Evaluation script
â”‚   â””â”€â”€ compare_strategies.py    # Academic comparison
â”œâ”€â”€ modeling/                    # SEEM model files (from original repo)
â””â”€â”€ utils/                       # SEEM utilities (from original repo)
```

## ğŸš€ Installation

### 1. Clone the repository
```bash
git clone https://github.com/yourusername/seem-octa.git
cd seem-octa
```

### 2. Create conda environment
```bash
conda create -n seem-octa python=3.9 -y
conda activate seem-octa
```

### 3. Install dependencies
```bash
pip install -r requirements.txt
```

### 4. Download SEEM pretrained weights
```bash
wget https://huggingface.co/xdecoder/SEEM/resolve/main/seem_focall_v0.pt
```

## ğŸ“Š Data Preparation

Organize your OCTA-500 dataset as follows:
```
octa/
â”œâ”€â”€ OCTA_3mm/
â”‚   â”œâ”€â”€ OCTA(FULL)/
â”‚   â”œâ”€â”€ OCTA(ILM_OPL)/
â”‚   â””â”€â”€ OCTA(OPL_BM)/
â”œâ”€â”€ OCTA_6mm/
â”‚   â”œâ”€â”€ OCTA(FULL)/
â”‚   â”œâ”€â”€ OCTA(ILM_OPL)/
â”‚   â””â”€â”€ OCTA(OPL_BM)/
â””â”€â”€ Label/
    â””â”€â”€ Label/
        â””â”€â”€ GT_LargeVessel/
```

## ğŸ‹ï¸ Training

### Train with GIIR Strategy (Recommended)
```bash
python scripts/train.py --strategy giir --epochs 50 --batch-size 2
```

### Train with Random Strategy (Baseline)
```bash
python scripts/train.py --strategy random --epochs 50 --batch-size 2
```

### Training Options
```bash
python scripts/train.py --help
```

## ğŸ“ˆ Evaluation

### Run Academic Comparison
```bash
python scripts/compare_strategies.py \
    --weights checkpoints/best_model_merged.pth \
    --num-samples 20 \
    --num-seeds 5
```

### Evaluate Single Model
```bash
python scripts/evaluate.py \
    --weights checkpoints/best_model.pth \
    --strategy giir
```

## ğŸ“‹ Results

### OCTA-500 3mm Dataset

| Method | Dice | IoU | clDice | Avg Clicks | Params (M) |
|--------|------|-----|--------|------------|------------|
| SEEM Baseline | 0.8756 | 0.7821 | 0.8412 | 5.58 | 31.2 |
| **GIIR (Ours)** | **0.9109** | **0.8389** | **0.8847** | **3.35** | **1.25** |

### Key Findings

1. **40% Click Reduction**: GIIR reduces required clicks from 5.58 to 3.35
2. **Parameter Efficiency**: Only 1.25% trainable parameters via LoRA
3. **Basin of Attraction Effect**: Geometry-informed training improves performance even with random inference

## ğŸ”¬ Method Overview

### GIIR (Geometry-Informed Interactive Refinement)

1. **Deterministic Click Selection**: Uses distance transform to find optimal click locations
2. **Oracle Mask Selection**: Selects best mask based on GT overlap during training
3. **Local Refinement**: Combines global structure with click-guided local corrections

### Training Strategy

```
For each sample:
    1. Get initial prediction (0 clicks)
    2. For click in 1..10:
        a. Find largest error region (FN or FP)
        b. Select click point using distance transform (GIIR) or random (baseline)
        c. Forward pass with accumulated clicks
        d. Compute loss and backpropagate
```

## ğŸ“š Citation

If you use this code, please cite:

```bibtex
@article{jaafar2025seemocta,
  title={SEEM-OCTA: Parameter-Efficient Interactive Vessel Segmentation with Geometry-Informed Refinement},
  author={Jaafar, et al.},
  journal={arXiv preprint},
  year={2025}
}
```

## ğŸ™ Acknowledgments

- [SEEM](https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once) for the base model
- [OCTA-500](https://ieee-dataport.org/open-access/octa-500) dataset
- Iran University of Science and Technology

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
